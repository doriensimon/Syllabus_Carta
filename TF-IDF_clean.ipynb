{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as pdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "from tkinter import Tk, filedialog\n",
    "from tokenize import tokenize\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "import io\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import win32com.client\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "import textract\n",
    "import docx\n",
    "import os\n",
    "import re\n",
    "from tkinter import Tk, filedialog\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = frozenset(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick File where syllabai are located "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = Tk()\n",
    "root.directory = filedialog.askdirectory()\n",
    "directory = root.directory\n",
    "root.withdraw() #hide the main window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-d9e1bbf5e955>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_pdf_to_txt_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;34m'.doc'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfirst_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtm_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-174-836e5477397a>\u001b[0m in \u001b[0;36mdoc_parser\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdoc_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwin32com\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtext_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\win32com\\client\\__init__.py\u001b[0m in \u001b[0;36mGetObject\u001b[1;34m(Pathname, Class, clsctx)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mGetActiveObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclsctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mMoniker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclsctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mGetActiveObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mClass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclsctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpythoncom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLSCTX_ALL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Lib\\site-packages\\win32com\\client\\__init__.py\u001b[0m in \u001b[0;36mMoniker\u001b[1;34m(Pathname, clsctx)\u001b[0m\n\u001b[0;32m     86\u001b[0m   \"\"\"\n\u001b[0;32m     87\u001b[0m   \u001b[0mmoniker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbindCtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpythoncom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMkParseDisplayName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPathname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m   \u001b[0mdispatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmoniker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBindToObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbindCtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpythoncom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIID_IDispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m__WrapDispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclsctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclsctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize a list to add parsed text to\n",
    "text_list = []\n",
    "\n",
    "# Loop through the files in the directory\n",
    "for filename in os.listdir(directory)[:500]: #only did 500 becuase it takes forever\n",
    "    first_file = filename\n",
    "    path = directory + '/' + first_file\n",
    "    \n",
    "    #handle parsing for different file types\n",
    "    if '.pdf' in first_file:\n",
    "        text = convert_pdf_to_txt_2(path)\n",
    "    elif '.doc' in first_file:\n",
    "        text = doc_parser(path)\n",
    "    else:\n",
    "        text = htm_parser(path)\n",
    "    text_list.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Processing of Data to create TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_df=0.85,stop_words=stop_words)\n",
    "word_count_vector=cv.fit_transform(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['african',\n",
       " 'middle',\n",
       " 'eastern',\n",
       " 'languages',\n",
       " 'literatures',\n",
       " 'language',\n",
       " 'center',\n",
       " 'university',\n",
       " 'amelang',\n",
       " 'beginning']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the keyword generation for document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=cv.get_feature_names()\n",
    "number = np.random.randint(0, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "test_list = []\n",
    "for filename in os.listdir(directory):\n",
    "    if count == number:\n",
    "        test_file = filename\n",
    "        path = directory + '/' + test_file\n",
    "        if '.pdf' in test_file:\n",
    "            text = convert_pdf_to_txt_2(path)\n",
    "        elif '.doc' in test_file:\n",
    "            text = doc_parser(path)\n",
    "        else:\n",
    "            text = htm_parser(path)\n",
    "        test_list.append(text)\n",
    "    count+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      " mark twain and the assault of laughter stanford university autumn english t mw hilton obenzinger office hours m w pm pm and by appointment office tba office phone tba home phone email obenzinger stanford edu for your human race in its poverty has unquestionably one really effective weapon laughter power money persuasion supplication persecution these can lift at a colossal humbug push it a little weaken it a little century by century but only laughter can blow it to rags and atoms at a blast against the assault of laughter nothing can stand from the mysterious stranger manuscripts description mark twain continues to delight and to disturb in this course we will examine how an outrageous phunny phellow notorious liar and irreverent blasphemer became a moral barometer the abraham lincoln of our literature against the assault of laughter nothing can stand twain wrote and his fictions satires and burlesques provided a constant comic barrage against the pretensions of his day major works such as huckleberry finn and pudd nhead wilson as well as less known works such as no the mysterious stranger we will focus on both twain s literary and comic techniques and how his complex often mythic fictions erupt along america s fault lines of race gender and class required texts tales speeches essays and sketches the innocents abroad roughing it the adventures of tom sawyer adventures of huckleberry finn a connecticut yankee in king arthur s court pudd nhead wilson no the mysterious stranger the bible according to mark twain class requirements classroom participation the class is a seminar which means that students will play an active role in the explication of texts i will provide contexts and interpretations but students will challenge and expand upon what i offer i expect all students to get involved in the discussions students will make presentations to the seminar preparing them in consultation with me consequently class participation including regular attendance active discussion and presenations accounts for percent of your grade critical essays there will be two papers both no less than pages words long valued at percent students will not be assigned topics but will develop their own essay topics although you are encouraged to consult with me for suggestions and feedback neither of these two papers can be about huckleberry finn because interpretive audio project students will also produce an interpretive audio project of huckleberry finn presenting a dramatic reading of a selection or edited selections from the novel at least minutes long students will write at least pages words analyzing their selection its significance within the novel what it reveals about the characters how irony does or does not operate in it and so forth and interpreting their own reading why the voices are done in such manner accents characterizations inflections editing this is not an acting class so criteria will be adjusted accordingly the interpretive audio project should be recorded on cd or some other compatible equipment to be discussed the project accounts for percent of your grade class schedule week m introduction w tall tales hoaxes and lies jim smiley and his jumping frog whittier birthday speech the babies as they comfort us in our sorrows let us not forget them in our festivities private history of a campaign that failed how to tell a story the turning point of my life all in tales artemus ward selections especially high handed outrage at utica http etext virginia edu railton innocent award html http library beau org gutenberg h h htm browne week m the innocents abroad especially preface chs end w innocents abroad week m roughing it chs fenimore cooper s literary offenses tales first four chapters of deadwood dick s doom or calamity jane s last adventure on penny dreadfuls web site www sul stanford edu depts dp pennies home html w roughing it week m tom sawyer the christmas fireside the story of the bad little boy that bore a charmed life story of the good little boy who did not prosper life as i find it tales excerpts from the story of a bad boy by thomas bailey aldrich http books google com books id bzmiaaaamaaj dq aldrich story of a bad boy pg pp ots muje ph a sig plhot aipzbbujtsmnbysy zxim hl en sa x oi book_result resnum ct result ppa m excerpts from ragged dick by horatio alger http nationalhumanitiescenter org pds gilded people text alger pdf w tom sawyer week m huckleberry finn through ch my first lie and how i got out of it tales huckleberry finn through ch sociable jimmy a true story tales excerpts on blackface minstrelsy http etext virginia edu railton huckfinn minstrl html http www iath virginia edu utc minstrel misohp html w huckleberry finn week m first paper due huckleberry finn w huckleberry finn week m a connecticut yankee in king arthur s court the facts concerning the recent carnival of crime in connecticut tales excerpts from looking backward by edward bellamy http xroads virginia edu hyper bellamy toc html excerpts from plunkitt of tammany hall recorded by william riordon http www marxists org reference archive plunkett george tammany hall index htm w a connecticut yankee in king arthur s court week m huckleberry finn audio project due pudd nhead wilson w pudd nhead wilson week m morality and religion eve speaks little bessie tales extracts from adam s diary mt s bible eve s diary mt s bible reflections on religion mt s bible captain stormfield s visit to heaven and letters from the earth mt s bible w politics imperialism and race to the person sitting in darkness corn pone opinions the anglo saxon race handout a defense of general funston http www druglibrary org schaffer general twain deffunst htm comments on the moro massacre http www is wayne edu mnissani cr moro htm the war prayer http www lexrex com informed otherdocuments warprayer htm united states of lyncherdom http people virginia edu sfr enam e lyncherdom html concerning the jews http www fordham edu halsall mod twain jews html week thanksgiving week m no the mysterious stranger w summation whoopjamboreehoo second paper due friday \n",
      "\n",
      "===Keywords===\n",
      "huckleberry 0.342\n",
      "finn 0.342\n",
      "twain 0.214\n",
      "tales 0.213\n",
      "laughter 0.171\n",
      "bible 0.16\n",
      "mysterious 0.137\n",
      "connecticut 0.128\n",
      "pudd 0.122\n",
      "nhead 0.122\n"
     ]
    }
   ],
   "source": [
    "feature_names=cv.get_feature_names()\n",
    " \n",
    "# get the document that we want to extract keywords from\n",
    "doc=test_list[0]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    " \n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    " \n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    " \n",
    "# now print the results\n",
    "print(\"\\n=====Doc=====\")\n",
    "print(doc)\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Functions Defined below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt_2(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "        \n",
    "    fp.close()\n",
    "    device.close()\n",
    "    text = retstr.getvalue()\n",
    "    text_vec = text.split('\\n')\n",
    "    text_total =  ' '\n",
    "    for word in text_vec:\n",
    "        text_total += ' ' + word\n",
    "    retstr.close()\n",
    "    \n",
    "     # lowercase\n",
    "    text_total=text_total.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text_total=re.sub(\"<!--?.*?-->\",\"\",text_total)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text_total=re.sub(\"(\\\\d|\\\\W)+\",\" \",text_total)\n",
    "    \n",
    "    return text_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def htm_parser(path):\n",
    "    f=codecs.open(path, 'r', encoding='latin-1')\n",
    "    unsoup = f.read()\n",
    "    soup = BeautifulSoup(unsoup)\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    strips = list(soup.stripped_strings)\n",
    "    text_total= ''\n",
    "    for strip in strips:\n",
    "        if (strip != 'Ê') and('Ê' not in strip):\n",
    "            text_total += ' ' + strip.strip('\\n')\n",
    "     \n",
    "    # lowercase\n",
    "    text_total=text_total.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text_total=re.sub(\"<!--?.*?-->\",\"\",text_total)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text_total=re.sub(\"(\\\\d|\\\\W)+\",\" \",text_total)\n",
    "    \n",
    "    return text_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def doc_parser(path):\n",
    "    doc = win32com.client.GetObject(path)\n",
    "    text= doc.Range().Text.split('\\r')\n",
    "    text_total = ' '\n",
    "    for word in text:\n",
    "        if '\u0007' in word:\n",
    "            word.replace('\u0007', '')\n",
    "        if word != '':\n",
    "            text_total += ' ' + word\n",
    "            \n",
    "     # lowercase\n",
    "    text_total=text_total.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text_total=re.sub(\"<!--?.*?-->\",\"\",text_total)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text_total=re.sub(\"(\\\\d|\\\\W)+\",\" \",text_total)\n",
    "    \n",
    "    return text_total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
