{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running this cell, skip to the bottom and run all the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file management stuff\n",
    "from tkinter import Tk, filedialog\n",
    "import os\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# doc parsing\n",
    "import win32com.client\n",
    "\n",
    "import random\n",
    "\n",
    "# pdf parsing\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import io\n",
    "from io import StringIO\n",
    "\n",
    "# html parsing\n",
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import csv\n",
    "\n",
    "# to run parsing in parallel\n",
    "import concurrent.futures\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "porter=PorterStemmer()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from docx2pdf import convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the folder holding all of the syllabi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = Tk()\n",
    "root.directory = filedialog.askdirectory()\n",
    "directory = root.directory\n",
    "root.withdraw() #hide the main window "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this cell to select individual files for any parsing of single documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.filename = filedialog.askopenfilename()\n",
    "file = root.filename\n",
    "root.withdraw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('stopwords_jul.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        stopwords.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_course = []\n",
    "bad_files = open('bad_course_file.txt')\n",
    "for file in bad_files:\n",
    "    bad_course.append(file.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_list_doc = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename in bad_course:\n",
    "        continue\n",
    "    course_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllabi_df = {'Year': [], 'Section': [], 'Quarter': [], 'department': [], 'CLASS': [], 'syllabi': [], 'website_count': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in course_list:\n",
    "    \n",
    "    try: \n",
    "        text, count = parser(file)\n",
    "        \n",
    "    except:\n",
    "        if file not in bad_course:\n",
    "            bad_course.append(file)\n",
    "        continue\n",
    "        \n",
    "    choice = re.search('[a-zA-Z]', text)\n",
    "\n",
    "    if choice == None:\n",
    "        continue\n",
    "\n",
    "\n",
    "    season, year, depart, course_name, section_num = filename_parse(file)   \n",
    "\n",
    "    syllabi_df['Quarter'].append(season)\n",
    "    syllabi_df['Section'].append(section_num)\n",
    "    syllabi_df['Year'].append(year)\n",
    "    syllabi_df['department'].append(depart)\n",
    "    syllabi_df['CLASS'].append(course_name)\n",
    "    syllabi_df['syllabi'].append(text)\n",
    "    syllabi_df['website_count'].append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad_courses = open('bad_course_file.txt', 'a')\n",
    "for course in bad_course:\n",
    "    bad_courses.write(course)\n",
    "    bad_courses.write('\\n')\n",
    "bad_courses.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllabi_df = pd.DataFrame(syllabi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllabi_df.to_csv('syllabi_df_doc_no_stem.csv',index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Description Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_count = 0\n",
    "main_dict = {'SUBJECT': [], 'CLASS': [], 'CATALOG_NBR': [], 'CATALOG_DESCRIPTION': []}\n",
    "with open(file, newline = '', encoding='utf-8') as games:                                                                                          \n",
    "    game_reader = csv.reader(games, delimiter='\\t')\n",
    "    for game in game_reader:\n",
    "        game_count+= 1\n",
    "        if game_count >= 2:\n",
    "            if game[4] == 'NULL' or game[4] == 'TBA':\n",
    "                continue\n",
    "            \n",
    "            text = game[4]\n",
    "            text = re.sub(\"<!--?.*?-->\",\"\",text)\n",
    "            text = re.sub(\"(\\\\d|\\\\W)+\",\" \", text)\n",
    "            text = removeStopWords(text)\n",
    "            text = game[4].lower()\n",
    "#             text = stemSentence(text)            \n",
    "            \n",
    "            main_dict['SUBJECT'].append(game[0])\n",
    "            main_dict['CLASS'].append(game[0]+ '-' + game[1])\n",
    "            main_dict['CATALOG_NBR'].append(game[1])\n",
    "            main_dict['CATALOG_DESCRIPTION'].append(text)\n",
    "descr_df = pd.DataFrame(main_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_df.to_csv('description_df_no_stem.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_parse(filename):\n",
    "    first_dash = filename.find('-')\n",
    "    quart_year = filename[:first_dash]\n",
    "    second_dash = filename.find('-', first_dash + 1)\n",
    "    depart = filename[first_dash + 1: second_dash]\n",
    "    third_dash = filename.find('-', second_dash + 1)\n",
    "    course_name = filename[first_dash + 1: third_dash]\n",
    "    end = filename.find('.')\n",
    "    section_num = filename[third_dash + 1:end]\n",
    "    \n",
    "    \n",
    "    # Parsing the season out of the filename\n",
    "    if 'F' in quart_year:\n",
    "        season = 'Fall'\n",
    "    elif 'W' in quart_year:\n",
    "        season = 'Winter'\n",
    "    elif 'Sp' in quart_year:\n",
    "        season = 'Spring'\n",
    "    else:\n",
    "        season = 'Summer'\n",
    "        \n",
    "    \n",
    "    # parsing out the year out of the filename\n",
    "    numbers = '1234567890'\n",
    "    begin = 0\n",
    "    while quart_year[begin] not in numbers:\n",
    "        begin += 1\n",
    "    year = quart_year[begin:]\n",
    "    year = int('20' + year)\n",
    "\n",
    "    \n",
    "    \n",
    "    return season, year, depart, course_name, section_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(string):\n",
    "    word_list = string.split()\n",
    "    new_list = []\n",
    "    for word in word_list:\n",
    "        if word not in stopwords:\n",
    "            new_list.append(word)\n",
    "    return \" \".join(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(string): \n",
    "  \n",
    "    # findall() has been used  \n",
    "    # with valid conditions for urls in string \n",
    "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "    url = re.findall(regex,string)       \n",
    "    return [x[0] for x in url] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(file):\n",
    "    path = directory + '/' + file\n",
    "    if '.pdf' in file:\n",
    "        text, count = convert_pdf_to_txt_2(path)\n",
    "    elif '.doc' in file:\n",
    "        text, count = doc_parser(path)\n",
    "    else:\n",
    "        text, count = htm_parser(path)\n",
    "    return text, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt_2(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                  password=password,\n",
    "                                  caching=caching,\n",
    "                                  check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "        \n",
    "    fp.close()\n",
    "    device.close()\n",
    "    text = retstr.getvalue()\n",
    "    text_vec = text.split('\\n')\n",
    "    text_total =  ' '\n",
    "    for word in text_vec:\n",
    "        text_total += ' ' + word\n",
    "    retstr.close()\n",
    "    \n",
    "    \n",
    "    url_list = find_url(text_total)\n",
    "    email_list = re.findall('\\S+@\\S+', text_total)\n",
    "    \n",
    "    for url in url_list:\n",
    "        text_total = text_total.replace(url, \"\")\n",
    "    \n",
    "    for email in email_list:\n",
    "        text_total = text_total.replace(email, \"\")\n",
    "    \n",
    "    \n",
    "     # lowercase\n",
    "    text_total=text_total.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text_total=re.sub(\"<!--?.*?-->\",\"\",text_total)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \", text_total)\n",
    "    text = removeStopWords(text)\n",
    "#     text = stemSentence(text)\n",
    "    \n",
    "    \n",
    "    return text, len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def htm_parser(path):\n",
    "    f=codecs.open(path, 'r', encoding='latin-1')\n",
    "    unsoup = f.read()\n",
    "    soup = BeautifulSoup(unsoup)\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    strips = list(soup.stripped_strings)\n",
    "    text_total= ''\n",
    "    for strip in strips:\n",
    "        if (strip != 'Ê') and('Ê' not in strip):\n",
    "            text_total += ' ' + strip.strip('\\n')\n",
    "    \n",
    "    \n",
    "    url_list = find_url(text_total)\n",
    "    email_list = re.findall('\\S+@\\S+', text_total)\n",
    "    \n",
    "    for url in url_list:\n",
    "        text_total = text_total.replace(url, \"\")\n",
    "    \n",
    "    for email in email_list:\n",
    "        text_total = text_total.replace(email, \"\")\n",
    "    \n",
    "    \n",
    "    # lowercase\n",
    "    text_total=text_total.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text_total=re.sub(\"<!--?.*?-->\",\"\",text_total)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \", text_total)\n",
    "    text = removeStopWords(text)\n",
    "#     text = stemSentence(text)\n",
    "    \n",
    "    return text, len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def doc_parser(path):\n",
    "    doc = win32com.client.GetObject(path)\n",
    "    text= doc.Range().Text.split('\\r')\n",
    "    \n",
    "    text_total = ' '\n",
    "    for word in text:\n",
    "        if '\u0007' in word:\n",
    "            word = word.replace('\u0007', '')\n",
    "        if word != '':\n",
    "            text_total += ' ' + word \n",
    "    \n",
    "    \n",
    "    url_list = find_url(text_total)\n",
    "    email_list = re.findall('\\S+@\\S+', text_total)\n",
    "    \n",
    "    for url in url_list:\n",
    "        text_total = text_total.replace(url, \"\")\n",
    "    \n",
    "    for email in email_list:\n",
    "        text_total = text_total.replace(email, \"\")\n",
    "    \n",
    "     # lowercase\n",
    "    text_total=text_total.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text_total=re.sub(\"<!--?.*?-->\",\"\",text_total)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \", text_total)\n",
    "    text = removeStopWords(text)\n",
    "#     text = stemSentence(text)\n",
    "    \n",
    "    return text, len(url_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
